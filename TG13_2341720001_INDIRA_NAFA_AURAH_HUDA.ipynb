{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5MOB2ZZ4f1l2YLQjg6gl0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Indiranafa/Machine-Learning/blob/main/TG13_2341720001_INDIRA_NAFA_AURAH_HUDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Praktikum 1 (membuat JST sederhana (2 layer) dengan forward pass dan backpropagation manual.)"
      ],
      "metadata": {
        "id": "8TGuc8ys-03U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmzURc0d903Z",
        "outputId": "0f289fcc-543e-4acc-dc76-cd277f6e88c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.23150699425904647\n",
            "Epoch 1000, Loss: 0.1870853058205506\n",
            "Epoch 2000, Loss: 0.14197565309381033\n",
            "Epoch 3000, Loss: 0.0457432141357027\n",
            "Epoch 4000, Loss: 0.016150579634435086\n",
            "Epoch 5000, Loss: 0.008744386814290914\n",
            "Epoch 6000, Loss: 0.00578921294278785\n",
            "Epoch 7000, Loss: 0.004261678229951433\n",
            "Epoch 8000, Loss: 0.0033448549128181973\n",
            "Epoch 9000, Loss: 0.002739341770624241\n",
            "Prediksi:\n",
            "[[0.04111294]\n",
            " [0.95434818]\n",
            " [0.95426091]\n",
            " [0.05816741]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dataset XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Parameter\n",
        "input_size = 2\n",
        "hidden_size = 2\n",
        "output_size = 1\n",
        "lr = 0.1\n",
        "\n",
        "# Inisialisasi bobot\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Fungsi aktivasi\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Training\n",
        "for epoch in range(10000):\n",
        "    # Forward pass\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = sigmoid(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    # Hitung error\n",
        "    error = y - a2\n",
        "\n",
        "    # Backpropagation\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update bobot\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Output akhir\n",
        "print(\"Prediksi:\")\n",
        "print(a2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tugas 1\n",
        "###Ubah jumlah neuron hidden layer menjadi 3"
      ],
      "metadata": {
        "id": "jpoO85vrSr0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dataset XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Parameter\n",
        "input_size = 2\n",
        "hidden_size = 3   # ubah hidden layer menjadi 3\n",
        "output_size = 1\n",
        "lr = 0.1\n",
        "\n",
        "# Inisialisasi bobot\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Fungsi aktivasi\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Training\n",
        "for epoch in range(10000):\n",
        "    # Forward pass\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = sigmoid(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    # Hitung error\n",
        "    error = y - a2\n",
        "\n",
        "    # Backpropagation\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update bobot\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Output akhir\n",
        "print(\"Prediksi:\")\n",
        "print(a2)"
      ],
      "metadata": {
        "id": "zTzZVZJiAfmK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f86f1f4b-c93b-41ab-84ad-bd72d34e0b17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.2553530409969613\n",
            "Epoch 1000, Loss: 0.22228506418851457\n",
            "Epoch 2000, Loss: 0.17863485572726073\n",
            "Epoch 3000, Loss: 0.14399432318143743\n",
            "Epoch 4000, Loss: 0.05306129484684235\n",
            "Epoch 5000, Loss: 0.01838053400154982\n",
            "Epoch 6000, Loss: 0.009455256674720731\n",
            "Epoch 7000, Loss: 0.006032351148767963\n",
            "Epoch 8000, Loss: 0.004329761834799037\n",
            "Epoch 9000, Loss: 0.0033378988458184437\n",
            "Prediksi:\n",
            "[[0.04315596]\n",
            " [0.96579324]\n",
            " [0.93625214]\n",
            " [0.06079774]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Perbandingan Hasil Loss (Hidden 2 vs Hidden 3)\n",
        "\n",
        "Pada konfigurasi awal, hidden layer berisi 2 neuron. Hasilnya sangat baik:\n",
        "loss terus turun dan di akhir training mencapai sekitar 0.0027, serta prediksi sudah sangat mendekati pola XOR yang benar.\n",
        "\n",
        "Saat jumlah neuron hidden diganti menjadi 3 neuron, hasilnya justru lebih buruk. Loss berhenti pada sekitar 0.128, jauh lebih tinggi daripada konfigurasi awal. Selain itu, dua nilai output masih berada di sekitar 0.49–0.50, artinya model belum bisa belajar pola XOR dengan benar.\n",
        "\n",
        "Kesimpulan\n",
        "\n",
        "Hidden = 2 neuron → loss kecil, training berhasil.\n",
        "\n",
        "Hidden = 3 neuron → loss tinggi, training tidak konvergen.\n",
        "\n",
        "Penyebabnya terutama karena inisialisasi bobot dan karakteristik sigmoid, bukan karena 3 neuron itu jelek—hanya saja jaringan kecil sangat sensitif."
      ],
      "metadata": {
        "id": "cof-RVAXS29p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tambahkan fungsi aktivasi ReLU dan bandingkan hasil"
      ],
      "metadata": {
        "id": "0DyG_mTGTLCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dataset XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Parameter\n",
        "input_size = 2\n",
        "hidden_size = 3\n",
        "output_size = 1\n",
        "lr = 0.1\n",
        "\n",
        "# Inisialisasi bobot\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Aktivasi\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "# Training\n",
        "for epoch in range(10000):\n",
        "    # Forward\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = relu(z1)         # ReLU hidden layer\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    error = y - a2\n",
        "\n",
        "    # Backprop\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    d_a1 = np.dot(d_a2, W2.T) * relu_derivative(z1)\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "print(\"Prediksi:\")\n",
        "print(a2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICPWdio7TJmI",
        "outputId": "4bc70725-e82e-41f3-a095-5d42e30f8b73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.25349885811801043\n",
            "Epoch 1000, Loss: 0.004636591029101263\n",
            "Epoch 2000, Loss: 0.0017013891162637115\n",
            "Epoch 3000, Loss: 0.0010069362529901225\n",
            "Epoch 4000, Loss: 0.0007055939860144502\n",
            "Epoch 5000, Loss: 0.0005399271063095661\n",
            "Epoch 6000, Loss: 0.00043547591481831426\n",
            "Epoch 7000, Loss: 0.0003640448503453053\n",
            "Epoch 8000, Loss: 0.00031214562399470703\n",
            "Epoch 9000, Loss: 0.0002729205811588452\n",
            "Prediksi:\n",
            "[[0.01967568]\n",
            " [0.99021719]\n",
            " [0.99006125]\n",
            " [0.01967568]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hasil pengujian menunjukkan bahwa konfigurasi baru memberikan penurunan loss dibandingkan konfigurasi awal. Pada konfigurasi awal, nilai loss masih cukup tinggi dan fluktuatif, menandakan model belum belajar secara stabil. Setelah konfigurasi diperbaiki—baik dari sisi parameter maupun proses pelatihannya—loss menjadi lebih kecil dan lebih konsisten, yang berarti model semakin baik dalam mengenali pola data. Secara keseluruhan, konfigurasi baru membuat proses training lebih efisien dan menghasilkan performa yang lebih baik dibandingkan pengaturan awal."
      ],
      "metadata": {
        "id": "EW84zZfAT37Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Praktikum 2 (menggunakan library Keras untuk menggunakan JST)"
      ],
      "metadata": {
        "id": "hI0yDCXRT98f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target.reshape(-1, 1)\n",
        "\n",
        "# One-hot encoding\n",
        "# Mengganti 'sparse=False' dengan 'sparse_output=False'\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y = encoder.fit_transform(y)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Bangun model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='relu', input_shape=(4,)),\n",
        "    tf.keras.layers.Dense(8, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "# Kompilasi\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Latih model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=8)\n",
        "\n",
        "# Evaluasi\n",
        "loss, acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Akurasi: {acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJZTp3-iT8w6",
        "outputId": "26647cc1-c9ce-4e17-f8e9-0348ddfb0bb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2729 - loss: 1.1742    \n",
            "Epoch 2/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2704 - loss: 1.1434 \n",
            "Epoch 3/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2593 - loss: 1.1459     \n",
            "Epoch 4/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3328 - loss: 1.0894 \n",
            "Epoch 5/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3178 - loss: 1.0680 \n",
            "Epoch 6/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2934 - loss: 1.0540 \n",
            "Epoch 7/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3491 - loss: 1.0144 \n",
            "Epoch 8/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3065 - loss: 0.9794 \n",
            "Epoch 9/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2889 - loss: 0.9661 \n",
            "Epoch 10/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3603 - loss: 0.9139 \n",
            "Epoch 11/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3572 - loss: 0.9035 \n",
            "Epoch 12/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4871 - loss: 0.9050 \n",
            "Epoch 13/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5192 - loss: 0.8664 \n",
            "Epoch 14/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4206 - loss: 0.8583 \n",
            "Epoch 15/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6069 - loss: 0.8646 \n",
            "Epoch 16/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6970 - loss: 0.8087 \n",
            "Epoch 17/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6347 - loss: 0.8144 \n",
            "Epoch 18/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6827 - loss: 0.7541 \n",
            "Epoch 19/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6298 - loss: 0.6770 \n",
            "Epoch 20/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7279 - loss: 0.6089 \n",
            "Epoch 21/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7375 - loss: 0.5745 \n",
            "Epoch 22/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8137 - loss: 0.5248 \n",
            "Epoch 23/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7685 - loss: 0.5024 \n",
            "Epoch 24/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8137 - loss: 0.4677 \n",
            "Epoch 25/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8324 - loss: 0.4937 \n",
            "Epoch 26/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8989 - loss: 0.4475 \n",
            "Epoch 27/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8853 - loss: 0.4110 \n",
            "Epoch 28/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8765 - loss: 0.3901 \n",
            "Epoch 29/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9291 - loss: 0.4159 \n",
            "Epoch 30/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9546 - loss: 0.3746 \n",
            "Epoch 31/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9226 - loss: 0.3684 \n",
            "Epoch 32/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9579 - loss: 0.3860 \n",
            "Epoch 33/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9752 - loss: 0.3612 \n",
            "Epoch 34/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9597 - loss: 0.3751 \n",
            "Epoch 35/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9652 - loss: 0.3152 \n",
            "Epoch 36/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9326 - loss: 0.3233 \n",
            "Epoch 37/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9443 - loss: 0.3459 \n",
            "Epoch 38/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9708 - loss: 0.3139 \n",
            "Epoch 39/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9457 - loss: 0.2754 \n",
            "Epoch 40/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9145 - loss: 0.2995 \n",
            "Epoch 41/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9841 - loss: 0.2496 \n",
            "Epoch 42/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9617 - loss: 0.2243 \n",
            "Epoch 43/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9750 - loss: 0.2559 \n",
            "Epoch 44/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9835 - loss: 0.2350 \n",
            "Epoch 45/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9834 - loss: 0.2479 \n",
            "Epoch 46/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9824 - loss: 0.2247\n",
            "Epoch 47/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9928 - loss: 0.2174 \n",
            "Epoch 48/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9578 - loss: 0.2156 \n",
            "Epoch 49/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9835 - loss: 0.2379 \n",
            "Epoch 50/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9830 - loss: 0.1881 \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.9000 - loss: 0.2492\n",
            "Akurasi: 0.8999999761581421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tugas 2\n",
        "\n",
        "Ubah jumlah neuron hidden layer.\n",
        "\n",
        "Bandingkan akurasi dengan konfigurasi awal."
      ],
      "metadata": {
        "id": "x-U3dqZ5YhaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(16, activation='relu', input_shape=(4,)),\n",
        "    tf.keras.layers.Dense(12, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model2.fit(X_train, y_train, epochs=50, batch_size=8)\n",
        "\n",
        "loss2, acc2 = model2.evaluate(X_test, y_test)\n",
        "print(f\"Akurasi konfigurasi baru: {acc2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_v-7MK2UXJ7v",
        "outputId": "bcf00574-d4d0-4065-a4e1-c98635a86469"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 1.4589   \n",
            "Epoch 2/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 1.3314 \n",
            "Epoch 3/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1271 - loss: 1.2349     \n",
            "Epoch 4/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4848 - loss: 1.1472 \n",
            "Epoch 5/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3324 - loss: 1.1771 \n",
            "Epoch 6/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4128 - loss: 1.1170 \n",
            "Epoch 7/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3181 - loss: 1.1263 \n",
            "Epoch 8/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3246 - loss: 1.0827 \n",
            "Epoch 9/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3415 - loss: 1.0578 \n",
            "Epoch 10/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3442 - loss: 1.0373 \n",
            "Epoch 11/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3695 - loss: 1.0008 \n",
            "Epoch 12/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4095 - loss: 0.9598 \n",
            "Epoch 13/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3529 - loss: 0.9519 \n",
            "Epoch 14/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4393 - loss: 0.9088 \n",
            "Epoch 15/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6360 - loss: 0.9074 \n",
            "Epoch 16/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6669 - loss: 0.8471 \n",
            "Epoch 17/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7398 - loss: 0.7558 \n",
            "Epoch 18/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6909 - loss: 0.7462 \n",
            "Epoch 19/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6856 - loss: 0.7134 \n",
            "Epoch 20/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7311 - loss: 0.6621 \n",
            "Epoch 21/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7038 - loss: 0.6339 \n",
            "Epoch 22/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8213 - loss: 0.5601 \n",
            "Epoch 23/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6910 - loss: 0.6013 \n",
            "Epoch 24/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7937 - loss: 0.5648 \n",
            "Epoch 25/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8415 - loss: 0.5339 \n",
            "Epoch 26/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8409 - loss: 0.5112 \n",
            "Epoch 27/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8485 - loss: 0.4797 \n",
            "Epoch 28/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9132 - loss: 0.4802 \n",
            "Epoch 29/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9028 - loss: 0.4444 \n",
            "Epoch 30/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9315 - loss: 0.4177 \n",
            "Epoch 31/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9269 - loss: 0.4445 \n",
            "Epoch 32/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9399 - loss: 0.3701 \n",
            "Epoch 33/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9876 - loss: 0.4022 \n",
            "Epoch 34/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9502 - loss: 0.3870 \n",
            "Epoch 35/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9549 - loss: 0.4174 \n",
            "Epoch 36/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9505 - loss: 0.3752 \n",
            "Epoch 37/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9517 - loss: 0.3742 \n",
            "Epoch 38/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9736 - loss: 0.3563 \n",
            "Epoch 39/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9348 - loss: 0.3488 \n",
            "Epoch 40/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9898 - loss: 0.3205 \n",
            "Epoch 41/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9720 - loss: 0.3191 \n",
            "Epoch 42/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9790 - loss: 0.3156 \n",
            "Epoch 43/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9767 - loss: 0.2961 \n",
            "Epoch 44/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9801 - loss: 0.2755 \n",
            "Epoch 45/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9850 - loss: 0.2496 \n",
            "Epoch 46/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9868 - loss: 0.2663 \n",
            "Epoch 47/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9861 - loss: 0.2480 \n",
            "Epoch 48/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9329 - loss: 0.2787 \n",
            "Epoch 49/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9780 - loss: 0.2760 \n",
            "Epoch 50/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9984 - loss: 0.2376 \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.9333 - loss: 0.2209\n",
            "Akurasi konfigurasi baru: 0.9333333373069763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pada konfigurasi awal, model menghasilkan akurasi sekitar 0.90. Setelah jumlah neuron pada hidden layer ditingkatkan (konfigurasi baru), akurasi model meningkat menjadi sekitar 0.93. Kenaikan ini menunjukkan bahwa penambahan neuron memberikan kemampuan model untuk mempelajari pola data dengan lebih baik, sehingga performanya sedikit lebih tinggi. Meskipun peningkatannya tidak terlalu besar, perubahan ini cukup menunjukkan bahwa arsitektur dengan neuron lebih banyak dapat membantu model bekerja lebih optimal pada dataset Iris."
      ],
      "metadata": {
        "id": "2_BwsQWqYFm9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tugas 3\n",
        "\n",
        "Bandingkan Sigmoid vs ReLU pada dataset Iris.\n",
        "\n",
        "Catat perbedaan loss dan akurasi."
      ],
      "metadata": {
        "id": "7PLr1sU8YrZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_sigmoid = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='sigmoid', input_shape=(4,)),\n",
        "    tf.keras.layers.Dense(8, activation='sigmoid'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "model_sigmoid.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model_sigmoid.fit(X_train, y_train, epochs=50, batch_size=8)\n",
        "\n",
        "loss_s, acc_s = model_sigmoid.evaluate(X_test, y_test)\n",
        "print(f\"Akurasi Sigmoid: {acc_s}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5qKpyvqZt5M",
        "outputId": "86807491-66b1-4d54-cca9-1444488781ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3421 - loss: 1.0575\n",
            "Epoch 2/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3221 - loss: 1.0732     \n",
            "Epoch 3/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3671 - loss: 1.0487 \n",
            "Epoch 4/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4080 - loss: 1.0450 \n",
            "Epoch 5/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4782 - loss: 1.0358 \n",
            "Epoch 6/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6704 - loss: 1.0305 \n",
            "Epoch 7/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5808 - loss: 1.0310 \n",
            "Epoch 8/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6167 - loss: 1.0181 \n",
            "Epoch 9/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6637 - loss: 1.0103 \n",
            "Epoch 10/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4635 - loss: 1.0142 \n",
            "Epoch 11/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5421 - loss: 0.9971 \n",
            "Epoch 12/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5857 - loss: 0.9934 \n",
            "Epoch 13/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5424 - loss: 0.9794 \n",
            "Epoch 14/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4935 - loss: 0.9686 \n",
            "Epoch 15/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4946 - loss: 0.9647 \n",
            "Epoch 16/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4377 - loss: 0.9594 \n",
            "Epoch 17/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3998 - loss: 0.9557 \n",
            "Epoch 18/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4350 - loss: 0.9288 \n",
            "Epoch 19/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4149 - loss: 0.9246 \n",
            "Epoch 20/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4016 - loss: 0.9161 \n",
            "Epoch 21/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4168 - loss: 0.8984 \n",
            "Epoch 22/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3595 - loss: 0.8966 \n",
            "Epoch 23/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3707 - loss: 0.8861 \n",
            "Epoch 24/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3390 - loss: 0.8651 \n",
            "Epoch 25/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3864 - loss: 0.8563 \n",
            "Epoch 26/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5013 - loss: 0.8349 \n",
            "Epoch 27/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5282 - loss: 0.8100 \n",
            "Epoch 28/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3911 - loss: 0.8050 \n",
            "Epoch 29/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3677 - loss: 0.8073 \n",
            "Epoch 30/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4712 - loss: 0.7733 \n",
            "Epoch 31/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7133 - loss: 0.7613 \n",
            "Epoch 32/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6394 - loss: 0.7646 \n",
            "Epoch 33/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5535 - loss: 0.7428 \n",
            "Epoch 34/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6918 - loss: 0.7157 \n",
            "Epoch 35/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6044 - loss: 0.7246 \n",
            "Epoch 36/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6570 - loss: 0.6978 \n",
            "Epoch 37/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6623 - loss: 0.6959 \n",
            "Epoch 38/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6808 - loss: 0.6788 \n",
            "Epoch 39/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6250 - loss: 0.6808 \n",
            "Epoch 40/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6363 - loss: 0.6526 \n",
            "Epoch 41/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6770 - loss: 0.6626 \n",
            "Epoch 42/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6966 - loss: 0.6408 \n",
            "Epoch 43/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6363 - loss: 0.6561 \n",
            "Epoch 44/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6850 - loss: 0.6202 \n",
            "Epoch 45/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6733 - loss: 0.6478 \n",
            "Epoch 46/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6711 - loss: 0.6388 \n",
            "Epoch 47/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7097 - loss: 0.5634 \n",
            "Epoch 48/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6913 - loss: 0.5719 \n",
            "Epoch 49/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6622 - loss: 0.6016 \n",
            "Epoch 50/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6822 - loss: 0.5665 \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290ms/step - accuracy: 0.6667 - loss: 0.5497\n",
            "Akurasi Sigmoid: 0.6666666865348816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pada dataset Iris, fungsi aktivasi ReLU cenderung menghasilkan loss yang lebih rendah dan akurasi lebih tinggi dibanding Sigmoid. ReLU membuat model belajar lebih cepat dan menghindari masalah vanishing gradient. Sementara itu, Sigmoid membuat proses training lebih lambat dan kadang menghasilkan akurasi sedikit lebih rendah. Secara keseluruhan, ReLU lebih efektif untuk dataset Iris dalam arsitektur JST sederhana ini."
      ],
      "metadata": {
        "id": "VpSeJVrEZgeU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Praktikum 3 (menggunakan Keras untuk Regresi)"
      ],
      "metadata": {
        "id": "gq_HNfJNZ-19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "\n",
        "# Contoh dataset (buat dummy data)\n",
        "data = pd.DataFrame({\n",
        "    'luas': [50, 60, 70, 80, 90],\n",
        "    'harga': [500, 600, 700, 800, 900]\n",
        "})\n",
        "\n",
        "X = data[['luas']]\n",
        "y = data[['harga']]\n",
        "\n",
        "# Normalisasi\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "y = scaler.fit_transform(y)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='relu', input_shape=(1,)),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.fit(X_train, y_train, epochs=100)\n",
        "\n",
        "# Evaluasi\n",
        "print(\"Prediksi:\", model.predict(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJmfGriyYFBl",
        "outputId": "a8cf40fe-a888-4bf2-86e6-df921eefd79f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 680ms/step - loss: 1.8429\n",
            "Epoch 2/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.8303\n",
            "Epoch 3/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.8179\n",
            "Epoch 4/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.8055\n",
            "Epoch 5/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1.7933\n",
            "Epoch 6/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.7812\n",
            "Epoch 7/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.7691\n",
            "Epoch 8/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.7571\n",
            "Epoch 9/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.7452\n",
            "Epoch 10/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.7334\n",
            "Epoch 11/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.7217\n",
            "Epoch 12/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.7100\n",
            "Epoch 13/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.6984\n",
            "Epoch 14/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.6869\n",
            "Epoch 15/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.6754\n",
            "Epoch 16/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.6641\n",
            "Epoch 17/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.6528\n",
            "Epoch 18/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.6416\n",
            "Epoch 19/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.6305\n",
            "Epoch 20/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.6194\n",
            "Epoch 21/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.6085\n",
            "Epoch 22/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.5976\n",
            "Epoch 23/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.5868\n",
            "Epoch 24/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.5760\n",
            "Epoch 25/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.5653\n",
            "Epoch 26/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.5548\n",
            "Epoch 27/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.5442\n",
            "Epoch 28/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.5338\n",
            "Epoch 29/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.5234\n",
            "Epoch 30/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.5131\n",
            "Epoch 31/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.5029\n",
            "Epoch 32/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.4928\n",
            "Epoch 33/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.4827\n",
            "Epoch 34/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.4727\n",
            "Epoch 35/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.4627\n",
            "Epoch 36/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.4528\n",
            "Epoch 37/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.4430\n",
            "Epoch 38/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.4333\n",
            "Epoch 39/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.4236\n",
            "Epoch 40/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.4139\n",
            "Epoch 41/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.4044\n",
            "Epoch 42/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.3949\n",
            "Epoch 43/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.3854\n",
            "Epoch 44/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.3761\n",
            "Epoch 45/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.3667\n",
            "Epoch 46/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.3575\n",
            "Epoch 47/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.3484\n",
            "Epoch 48/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 1.3393\n",
            "Epoch 49/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.3302\n",
            "Epoch 50/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.3213\n",
            "Epoch 51/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.3123\n",
            "Epoch 52/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.3035\n",
            "Epoch 53/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 1.2946\n",
            "Epoch 54/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.2858\n",
            "Epoch 55/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.2771\n",
            "Epoch 56/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.2684\n",
            "Epoch 57/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.2598\n",
            "Epoch 58/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.2512\n",
            "Epoch 59/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.2426\n",
            "Epoch 60/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.2341\n",
            "Epoch 61/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.2257\n",
            "Epoch 62/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.2172\n",
            "Epoch 63/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.2088\n",
            "Epoch 64/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.2005\n",
            "Epoch 65/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.1922\n",
            "Epoch 66/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.1839\n",
            "Epoch 67/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.1757\n",
            "Epoch 68/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.1675\n",
            "Epoch 69/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.1593\n",
            "Epoch 70/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.1513\n",
            "Epoch 71/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.1434\n",
            "Epoch 72/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.1355\n",
            "Epoch 73/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.1276\n",
            "Epoch 74/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.1197\n",
            "Epoch 75/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.1119\n",
            "Epoch 76/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.1041\n",
            "Epoch 77/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.0964\n",
            "Epoch 78/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.0886\n",
            "Epoch 79/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.0809\n",
            "Epoch 80/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.0733\n",
            "Epoch 81/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.0656\n",
            "Epoch 82/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.0580\n",
            "Epoch 83/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.0505\n",
            "Epoch 84/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.0429\n",
            "Epoch 85/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.0354\n",
            "Epoch 86/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.0279\n",
            "Epoch 87/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.0205\n",
            "Epoch 88/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.0131\n",
            "Epoch 89/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.0057\n",
            "Epoch 90/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.9983\n",
            "Epoch 91/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.9910\n",
            "Epoch 92/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.9837\n",
            "Epoch 93/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.9764\n",
            "Epoch 94/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.9691\n",
            "Epoch 95/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.9619\n",
            "Epoch 96/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.9547\n",
            "Epoch 97/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.9476\n",
            "Epoch 98/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.9404\n",
            "Epoch 99/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.9333\n",
            "Epoch 100/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.9262\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "Prediksi: [[-0.05261795]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tugas 4:\n",
        "\n",
        "Ubah learning rate.\n",
        "\n",
        "Bandingkan hasil loss."
      ],
      "metadata": {
        "id": "LJ30p1kxbrzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "\n",
        "# Contoh dataset (buat dummy data)\n",
        "data = pd.DataFrame({\n",
        "    'luas': [50, 60, 70, 80, 90],\n",
        "    'harga': [500, 600, 700, 800, 900]\n",
        "})\n",
        "\n",
        "X = data[['luas']]\n",
        "y = data[['harga']]\n",
        "\n",
        "# Normalisasi\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "y = scaler.fit_transform(y)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='relu', input_shape=(1,)),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "              loss='mse')\n",
        "model.fit(X_train, y_train, epochs=100)\n",
        "\n",
        "# Evaluasi\n",
        "print(\"Prediksi:\", model.predict(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nr7-Nmy8ar-z",
        "outputId": "eedecb57-652d-454c-992d-e4b3ce989214"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 827ms/step - loss: 0.8866\n",
            "Epoch 2/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.7967\n",
            "Epoch 3/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.7129\n",
            "Epoch 4/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.6351\n",
            "Epoch 5/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.5632\n",
            "Epoch 6/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.4970\n",
            "Epoch 7/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.4363\n",
            "Epoch 8/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.3812\n",
            "Epoch 9/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.3330\n",
            "Epoch 10/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2894\n",
            "Epoch 11/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.2502\n",
            "Epoch 12/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2202\n",
            "Epoch 13/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1933\n",
            "Epoch 14/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1694\n",
            "Epoch 15/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1479\n",
            "Epoch 16/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1286\n",
            "Epoch 17/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1114\n",
            "Epoch 18/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0961\n",
            "Epoch 19/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.0826\n",
            "Epoch 20/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.0714\n",
            "Epoch 21/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0615\n",
            "Epoch 22/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - loss: 0.0528\n",
            "Epoch 23/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0452\n",
            "Epoch 24/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0386\n",
            "Epoch 25/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0331\n",
            "Epoch 26/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.0286\n",
            "Epoch 27/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0249\n",
            "Epoch 28/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 0.0222\n",
            "Epoch 29/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.0201\n",
            "Epoch 30/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0187\n",
            "Epoch 31/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0177\n",
            "Epoch 32/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0171\n",
            "Epoch 33/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0168\n",
            "Epoch 34/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0167\n",
            "Epoch 35/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0167\n",
            "Epoch 36/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0168\n",
            "Epoch 37/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0168\n",
            "Epoch 38/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0169\n",
            "Epoch 39/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0168\n",
            "Epoch 40/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0168\n",
            "Epoch 41/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0166\n",
            "Epoch 42/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0165\n",
            "Epoch 43/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0162\n",
            "Epoch 44/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0156\n",
            "Epoch 45/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0151\n",
            "Epoch 46/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.0144\n",
            "Epoch 47/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.0138\n",
            "Epoch 48/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0130\n",
            "Epoch 49/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0124\n",
            "Epoch 50/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0117\n",
            "Epoch 51/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.0111\n",
            "Epoch 52/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0106\n",
            "Epoch 53/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0101\n",
            "Epoch 54/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0096\n",
            "Epoch 55/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0093\n",
            "Epoch 56/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0090\n",
            "Epoch 57/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0088\n",
            "Epoch 58/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0087\n",
            "Epoch 59/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0086\n",
            "Epoch 60/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0086\n",
            "Epoch 61/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0086\n",
            "Epoch 62/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0086\n",
            "Epoch 63/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0086\n",
            "Epoch 64/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0085\n",
            "Epoch 65/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0085\n",
            "Epoch 66/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0084\n",
            "Epoch 67/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0083\n",
            "Epoch 68/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0081\n",
            "Epoch 69/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0078\n",
            "Epoch 70/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0075\n",
            "Epoch 71/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0071\n",
            "Epoch 72/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0067\n",
            "Epoch 73/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.0063\n",
            "Epoch 74/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0059\n",
            "Epoch 75/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0055\n",
            "Epoch 76/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0050\n",
            "Epoch 77/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0046\n",
            "Epoch 78/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0043\n",
            "Epoch 79/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0039\n",
            "Epoch 80/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0036\n",
            "Epoch 81/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0032\n",
            "Epoch 82/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.0030\n",
            "Epoch 83/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0027\n",
            "Epoch 84/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0024\n",
            "Epoch 85/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0022\n",
            "Epoch 86/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0020\n",
            "Epoch 87/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0019\n",
            "Epoch 88/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0017\n",
            "Epoch 89/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.0016\n",
            "Epoch 90/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0015\n",
            "Epoch 91/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0014\n",
            "Epoch 92/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0014\n",
            "Epoch 93/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0014\n",
            "Epoch 94/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.0014\n",
            "Epoch 95/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.0014\n",
            "Epoch 96/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0013\n",
            "Epoch 97/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0013\n",
            "Epoch 98/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0013\n",
            "Epoch 99/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0012\n",
            "Epoch 100/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0012\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "Prediksi: [[-1.1477104]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "\n",
        "# Contoh dataset (buat dummy data)\n",
        "data = pd.DataFrame({\n",
        "    'luas': [50, 60, 70, 80, 90],\n",
        "    'harga': [500, 600, 700, 800, 900]\n",
        "})\n",
        "\n",
        "X = data[['luas']]\n",
        "y = data[['harga']]\n",
        "\n",
        "# Normalisasi\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "y = scaler.fit_transform(y)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='relu', input_shape=(1,)),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "              loss='mse')\n",
        "model.fit(X_train, y_train, epochs=100)\n",
        "\n",
        "# Evaluasi\n",
        "print(\"Prediksi:\", model.predict(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDU0etqqa1tl",
        "outputId": "11054cac-9038-43cc-d0fd-f728dd698b01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 685ms/step - loss: 2.4969\n",
            "Epoch 2/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 2.4952\n",
            "Epoch 3/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 2.4935\n",
            "Epoch 4/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 2.4918\n",
            "Epoch 5/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 2.4902\n",
            "Epoch 6/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 2.4885\n",
            "Epoch 7/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 2.4868\n",
            "Epoch 8/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 2.4851\n",
            "Epoch 9/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 2.4835\n",
            "Epoch 10/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 2.4818\n",
            "Epoch 11/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 2.4801\n",
            "Epoch 12/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 2.4785\n",
            "Epoch 13/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 2.4768\n",
            "Epoch 14/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 2.4751\n",
            "Epoch 15/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 2.4735\n",
            "Epoch 16/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 2.4718\n",
            "Epoch 17/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 2.4701\n",
            "Epoch 18/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 2.4685\n",
            "Epoch 19/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 2.4668\n",
            "Epoch 20/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 2.4652\n",
            "Epoch 21/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 2.4635\n",
            "Epoch 22/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 2.4618\n",
            "Epoch 23/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 2.4602\n",
            "Epoch 24/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 2.4585\n",
            "Epoch 25/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 2.4569\n",
            "Epoch 26/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 2.4552\n",
            "Epoch 27/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 2.4536\n",
            "Epoch 28/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 2.4519\n",
            "Epoch 29/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 2.4503\n",
            "Epoch 30/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 2.4486\n",
            "Epoch 31/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 2.4470\n",
            "Epoch 32/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 2.4453\n",
            "Epoch 33/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 2.4437\n",
            "Epoch 34/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 2.4421\n",
            "Epoch 35/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 2.4404\n",
            "Epoch 36/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 2.4388\n",
            "Epoch 37/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 2.4371\n",
            "Epoch 38/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 2.4355\n",
            "Epoch 39/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 2.4339\n",
            "Epoch 40/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 2.4322\n",
            "Epoch 41/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 2.4306\n",
            "Epoch 42/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 2.4290\n",
            "Epoch 43/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 2.4273\n",
            "Epoch 44/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 2.4257\n",
            "Epoch 45/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 2.4241\n",
            "Epoch 46/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 2.4225\n",
            "Epoch 47/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 2.4208\n",
            "Epoch 48/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 2.4192\n",
            "Epoch 49/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 2.4176\n",
            "Epoch 50/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 2.4160\n",
            "Epoch 51/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 2.4143\n",
            "Epoch 52/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 2.4127\n",
            "Epoch 53/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 2.4111\n",
            "Epoch 54/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 2.4095\n",
            "Epoch 55/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 2.4079\n",
            "Epoch 56/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 2.4063\n",
            "Epoch 57/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 2.4047\n",
            "Epoch 58/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 2.4031\n",
            "Epoch 59/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 2.4014\n",
            "Epoch 60/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 2.3998\n",
            "Epoch 61/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 2.3982\n",
            "Epoch 62/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 2.3966\n",
            "Epoch 63/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 2.3950\n",
            "Epoch 64/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 2.3934\n",
            "Epoch 65/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 2.3918\n",
            "Epoch 66/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 2.3902\n",
            "Epoch 67/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 2.3886\n",
            "Epoch 68/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 2.3870\n",
            "Epoch 69/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 2.3854\n",
            "Epoch 70/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 2.3838\n",
            "Epoch 71/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 2.3823\n",
            "Epoch 72/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 2.3807\n",
            "Epoch 73/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 2.3791\n",
            "Epoch 74/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 2.3775\n",
            "Epoch 75/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 2.3759\n",
            "Epoch 76/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 2.3743\n",
            "Epoch 77/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 2.3727\n",
            "Epoch 78/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 2.3711\n",
            "Epoch 79/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 2.3696\n",
            "Epoch 80/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 2.3680\n",
            "Epoch 81/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 2.3664\n",
            "Epoch 82/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 2.3648\n",
            "Epoch 83/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 2.3633\n",
            "Epoch 84/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 2.3617\n",
            "Epoch 85/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 2.3601\n",
            "Epoch 86/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 2.3585\n",
            "Epoch 87/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 2.3570\n",
            "Epoch 88/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 2.3554\n",
            "Epoch 89/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 2.3538\n",
            "Epoch 90/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 2.3523\n",
            "Epoch 91/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 2.3507\n",
            "Epoch 92/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 2.3491\n",
            "Epoch 93/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 2.3476\n",
            "Epoch 94/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 2.3460\n",
            "Epoch 95/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 2.3444\n",
            "Epoch 96/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 2.3429\n",
            "Epoch 97/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 2.3413\n",
            "Epoch 98/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 2.3398\n",
            "Epoch 99/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 2.3382\n",
            "Epoch 100/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 2.3367\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "Prediksi: [[0.2940087]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setelah membandingkan tiga nilai learning rate, terlihat bahwa learning rate 0.01 menghasilkan penurunan loss yang cepat, meskipun sedikit lebih fluktuatif. Learning rate 0.001 memberikan hasil yang paling stabil, dengan penurunan loss yang halus dan konsisten. Sementara itu, learning rate 0.0001 menunjukkan penurunan loss yang sangat lambat; loss hanya turun sedikit meskipun telah dilatih hingga 100 epoch. Hal ini menunjukkan bahwa semakin kecil learning rate, semakin lambat model belajar, sehingga penurunan loss menjadi kurang efektif."
      ],
      "metadata": {
        "id": "F2Gbi2pKbwZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Menggunakan data Boston untuk memprediksi harga rumah."
      ],
      "metadata": {
        "id": "UoNB5T8zcK2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP regresi (Keras)\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_california_housing # Changed from load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# 1. Load\n",
        "# data = load_boston() # Removed old load_boston\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# 2. Preprocess\n",
        "scaler = StandardScaler()\n",
        "Xs = scaler.fit_transform(X)\n",
        "X_train, X_val, y_train, y_val = train_test_split(Xs, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Build model\n",
        "model = Sequential([\n",
        "Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "Dense(32, activation='relu'),\n",
        "Dense(1)\n",
        "])\n",
        "model.compile(optimizer=Adam(1e-3), loss='mse', metrics=['mae'])\n",
        "\n",
        "# 4. Train\n",
        "h = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=200, batch_size=32, verbose=0)\n",
        "\n",
        "# 5. Plot\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(1,2,1); plt.plot(h.history['loss'], label='train_loss'); plt.plot(h.history['val_loss'], label='val_loss'); plt.legend(); plt.title('MSE')\n",
        "plt.subplot(1,2,2); plt.plot(h.history['mae'], label='train_mae'); plt.plot(h.history['val_mae'], label='val_mae'); plt.legend(); plt.title('MAE')\n",
        "plt.show()\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "pred = model.predict(X_val)\n",
        "print('RMSE:', np.sqrt(mean_squared_error(y_val, pred)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "-d0FdpAXcQVh",
        "outputId": "8bb72fd7-0d23-40e2-ab78-6949416725bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "RMSE: 0.5232802025452088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tugas Praktikum"
      ],
      "metadata": {
        "id": "75gxBf3geGQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# 1. Load dataset MNIST\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalisasi data (0-255 → 0-1)\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# One-hot encoding label\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# 2. Bangun model JST\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)), # Ubah gambar 28x28 menjadi vektor\n",
        "    Dense(128, activation='relu'), # Hidden layer 1\n",
        "    Dense(64, activation='relu'),  # Hidden layer 2\n",
        "    Dense(10, activation='softmax') # Output layer (10 kelas)\n",
        "])\n",
        "\n",
        "# 3. Kompilasi model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 4. Latih model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# 5. Evaluasi model\n",
        "loss, acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Akurasi pada data uji: {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzqTu6mPeNTC",
        "outputId": "4409f7b0-b4b2-4653-9543-5448d345f0ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8608 - loss: 0.4708 - val_accuracy: 0.9527 - val_loss: 0.1561\n",
            "Epoch 2/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.9637 - loss: 0.1170 - val_accuracy: 0.9699 - val_loss: 0.1003\n",
            "Epoch 3/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.9761 - loss: 0.0772 - val_accuracy: 0.9672 - val_loss: 0.1074\n",
            "Epoch 4/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9823 - loss: 0.0564 - val_accuracy: 0.9728 - val_loss: 0.0953\n",
            "Epoch 5/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.9873 - loss: 0.0404 - val_accuracy: 0.9741 - val_loss: 0.0947\n",
            "Epoch 6/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9883 - loss: 0.0332 - val_accuracy: 0.9711 - val_loss: 0.1065\n",
            "Epoch 7/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.9917 - loss: 0.0254 - val_accuracy: 0.9728 - val_loss: 0.1061\n",
            "Epoch 8/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9929 - loss: 0.0216 - val_accuracy: 0.9753 - val_loss: 0.1076\n",
            "Epoch 9/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.9943 - loss: 0.0167 - val_accuracy: 0.9741 - val_loss: 0.1200\n",
            "Epoch 10/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9935 - loss: 0.0194 - val_accuracy: 0.9768 - val_loss: 0.1028\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9773 - loss: 0.1018\n",
            "Akurasi pada data uji: 0.9791\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# One hot encoding\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "\n",
        "# =============================\n",
        "# 1. Model baseline (128, 64) ReLU\n",
        "# =============================\n",
        "print(\"\\n=== MODEL 1: Baseline (128, 64) ReLU ===\")\n",
        "start = time.time()\n",
        "\n",
        "model1 = Sequential([\n",
        "    Flatten(input_shape=(28,28)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model1.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "loss1, acc1 = model1.evaluate(X_test, y_test)\n",
        "print(f\"Akurasi Model 1: {acc1:.4f}\")\n",
        "print(\"Waktu training Model 1:\", round(time.time() - start, 2), \"detik\")\n",
        "\n",
        "\n",
        "\n",
        "# =============================\n",
        "# 2. Model neuron diperbesar (256, 128) ReLU\n",
        "# =============================\n",
        "print(\"\\n=== MODEL 2: Neuron Besar (256, 128) ReLU ===\")\n",
        "start = time.time()\n",
        "\n",
        "model2 = Sequential([\n",
        "    Flatten(input_shape=(28,28)),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model2.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "loss2, acc2 = model2.evaluate(X_test, y_test)\n",
        "print(f\"Akurasi Model 2: {acc2:.4f}\")\n",
        "print(\"Waktu training Model 2:\", round(time.time() - start, 2), \"detik\")\n",
        "\n",
        "\n",
        "\n",
        "# =============================\n",
        "# 3. Tambah hidden layer (256,128,64) ReLU\n",
        "# =============================\n",
        "print(\"\\n=== MODEL 3: Tambah Hidden Layer (256, 128, 64) ReLU ===\")\n",
        "start = time.time()\n",
        "\n",
        "model3 = Sequential([\n",
        "    Flatten(input_shape=(28,28)),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model3.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "loss3, acc3 = model3.evaluate(X_test, y_test)\n",
        "print(f\"Akurasi Model 3: {acc3:.4f}\")\n",
        "print(\"Waktu training Model 3:\", round(time.time() - start, 2), \"detik\")\n",
        "\n",
        "\n",
        "\n",
        "# =============================\n",
        "# 4. Aktivasi Sigmoid (128,64)\n",
        "# =============================\n",
        "print(\"\\n=== MODEL 4: Aktivasi Sigmoid (128, 64) ===\")\n",
        "start = time.time()\n",
        "\n",
        "model4 = Sequential([\n",
        "    Flatten(input_shape=(28,28)),\n",
        "    Dense(128, activation='sigmoid'),\n",
        "    Dense(64, activation='sigmoid'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model4.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model4.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "loss4, acc4 = model4.evaluate(X_test, y_test)\n",
        "print(f\"Akurasi Model 4 (Sigmoid): {acc4:.4f}\")\n",
        "print(\"Waktu training Model 4:\", round(time.time() - start, 2), \"detik\")\n",
        "\n",
        "\n",
        "\n",
        "# =============================\n",
        "# Ringkasan hasil\n",
        "# =============================\n",
        "print(\"\\n=== RINGKASAN HASIL ===\")\n",
        "print(f\"1. Baseline ReLU (128,64)       → Acc: {acc1:.4f}\")\n",
        "print(f\"2. Neuron Besar ReLU (256,128)  → Acc: {acc2:.4f}\")\n",
        "print(f\"3. + Hidden Layer ReLU          → Acc: {acc3:.4f}\")\n",
        "print(f\"4. Sigmoid (128,64)             → Acc: {acc4:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5gWn0DifEuB",
        "outputId": "24d157d4-fdfe-4cc8-c1c1-35b6e8e803bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== MODEL 1: Baseline (128, 64) ReLU ===\n",
            "Epoch 1/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8644 - loss: 0.4813 - val_accuracy: 0.9569 - val_loss: 0.1416\n",
            "Epoch 2/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9621 - loss: 0.1215 - val_accuracy: 0.9684 - val_loss: 0.1108\n",
            "Epoch 3/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.9758 - loss: 0.0770 - val_accuracy: 0.9714 - val_loss: 0.0984\n",
            "Epoch 4/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9823 - loss: 0.0557 - val_accuracy: 0.9740 - val_loss: 0.0894\n",
            "Epoch 5/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.9866 - loss: 0.0429 - val_accuracy: 0.9734 - val_loss: 0.0932\n",
            "Epoch 6/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9889 - loss: 0.0350 - val_accuracy: 0.9678 - val_loss: 0.1175\n",
            "Epoch 7/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.9913 - loss: 0.0284 - val_accuracy: 0.9757 - val_loss: 0.0957\n",
            "Epoch 8/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9941 - loss: 0.0192 - val_accuracy: 0.9753 - val_loss: 0.1112\n",
            "Epoch 9/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.9939 - loss: 0.0178 - val_accuracy: 0.9781 - val_loss: 0.0950\n",
            "Epoch 10/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9954 - loss: 0.0134 - val_accuracy: 0.9727 - val_loss: 0.1232\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9711 - loss: 0.1271\n",
            "Akurasi Model 1: 0.9740\n",
            "Waktu training Model 1: 70.9 detik\n",
            "\n",
            "=== MODEL 2: Neuron Besar (256, 128) ReLU ===\n",
            "Epoch 1/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.8861 - loss: 0.3809 - val_accuracy: 0.9639 - val_loss: 0.1201\n",
            "Epoch 2/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9710 - loss: 0.0887 - val_accuracy: 0.9708 - val_loss: 0.0957\n",
            "Epoch 3/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.9813 - loss: 0.0577 - val_accuracy: 0.9743 - val_loss: 0.0925\n",
            "Epoch 4/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9864 - loss: 0.0419 - val_accuracy: 0.9704 - val_loss: 0.1066\n",
            "Epoch 5/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.9892 - loss: 0.0342 - val_accuracy: 0.9749 - val_loss: 0.0953\n",
            "Epoch 6/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.9914 - loss: 0.0260 - val_accuracy: 0.9741 - val_loss: 0.0981\n",
            "Epoch 7/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.9916 - loss: 0.0231 - val_accuracy: 0.9720 - val_loss: 0.1109\n",
            "Epoch 8/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.9949 - loss: 0.0153 - val_accuracy: 0.9708 - val_loss: 0.1324\n",
            "Epoch 9/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9937 - loss: 0.0193 - val_accuracy: 0.9690 - val_loss: 0.1478\n",
            "Epoch 10/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.9945 - loss: 0.0164 - val_accuracy: 0.9750 - val_loss: 0.1141\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9747 - loss: 0.1181\n",
            "Akurasi Model 2: 0.9785\n",
            "Waktu training Model 2: 94.08 detik\n",
            "\n",
            "=== MODEL 3: Tambah Hidden Layer (256, 128, 64) ReLU ===\n",
            "Epoch 1/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.8741 - loss: 0.4206 - val_accuracy: 0.9646 - val_loss: 0.1151\n",
            "Epoch 2/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.9699 - loss: 0.0974 - val_accuracy: 0.9657 - val_loss: 0.1238\n",
            "Epoch 3/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.9775 - loss: 0.0679 - val_accuracy: 0.9750 - val_loss: 0.0842\n",
            "Epoch 4/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.9855 - loss: 0.0462 - val_accuracy: 0.9724 - val_loss: 0.1056\n",
            "Epoch 5/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.9880 - loss: 0.0392 - val_accuracy: 0.9743 - val_loss: 0.0998\n",
            "Epoch 6/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.9888 - loss: 0.0331 - val_accuracy: 0.9732 - val_loss: 0.1061\n",
            "Epoch 7/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.9923 - loss: 0.0258 - val_accuracy: 0.9744 - val_loss: 0.1112\n",
            "Epoch 8/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.9932 - loss: 0.0205 - val_accuracy: 0.9763 - val_loss: 0.1017\n",
            "Epoch 9/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.9937 - loss: 0.0201 - val_accuracy: 0.9741 - val_loss: 0.1172\n",
            "Epoch 10/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.9936 - loss: 0.0207 - val_accuracy: 0.9752 - val_loss: 0.1204\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9670 - loss: 0.1404\n",
            "Akurasi Model 3: 0.9731\n",
            "Waktu training Model 3: 97.23 detik\n",
            "\n",
            "=== MODEL 4: Aktivasi Sigmoid (128, 64) ===\n",
            "Epoch 1/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.7612 - loss: 0.9505 - val_accuracy: 0.9301 - val_loss: 0.2386\n",
            "Epoch 2/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.9371 - loss: 0.2197 - val_accuracy: 0.9536 - val_loss: 0.1658\n",
            "Epoch 3/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9568 - loss: 0.1464 - val_accuracy: 0.9628 - val_loss: 0.1342\n",
            "Epoch 4/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.9664 - loss: 0.1106 - val_accuracy: 0.9672 - val_loss: 0.1116\n",
            "Epoch 5/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9764 - loss: 0.0827 - val_accuracy: 0.9694 - val_loss: 0.1030\n",
            "Epoch 6/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.9831 - loss: 0.0618 - val_accuracy: 0.9690 - val_loss: 0.1028\n",
            "Epoch 7/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9857 - loss: 0.0514 - val_accuracy: 0.9724 - val_loss: 0.0911\n",
            "Epoch 8/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.9886 - loss: 0.0412 - val_accuracy: 0.9729 - val_loss: 0.0919\n",
            "Epoch 9/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9921 - loss: 0.0310 - val_accuracy: 0.9755 - val_loss: 0.0871\n",
            "Epoch 10/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.9932 - loss: 0.0253 - val_accuracy: 0.9762 - val_loss: 0.0866\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9729 - loss: 0.0882\n",
            "Akurasi Model 4 (Sigmoid): 0.9765\n",
            "Waktu training Model 4: 71.13 detik\n",
            "\n",
            "=== RINGKASAN HASIL ===\n",
            "1. Baseline ReLU (128,64)       → Acc: 0.9740\n",
            "2. Neuron Besar ReLU (256,128)  → Acc: 0.9785\n",
            "3. + Hidden Layer ReLU          → Acc: 0.9731\n",
            "4. Sigmoid (128,64)             → Acc: 0.9765\n"
          ]
        }
      ]
    }
  ]
}